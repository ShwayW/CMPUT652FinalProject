{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Collect trajectories from a trained agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines3 import PPO\n",
    "from mario_env import MarioEnv\n",
    "from stable_baselines3.common.vec_env import VecFrameStack, VecTransposeImage, DummyVecEnv\n",
    "from gym.wrappers import ResizeObservation\n",
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "from imitation.data import types, rollout\n",
    "\n",
    "# Generate environment and wrap it\n",
    "env = MarioEnv(render=False, horizons=True, sticky=True)\n",
    "env = ResizeObservation(env, 84)\n",
    "env = DummyVecEnv([lambda: env])\n",
    "env = VecFrameStack(env, n_stack=4)\n",
    "env = VecTransposeImage(env, skip=False)\n",
    "\n",
    "model = PPO.load(\"saved_agents/ppo_resize_sticky_500000_steps.zip\", env=env) # Load agent\n",
    "\n",
    "num_episodes = 100\n",
    "trajectories = []\n",
    "\n",
    "# for i in range(num_episodes):\n",
    "while len(trajectories) < num_episodes: # in case we discard bad samples\n",
    "    ep_acts, ep_obs, ep_rews, ep_dones, ep_infos = [], [], [], [], []\n",
    "    obs = env.reset()\n",
    "    ep_obs.append(obs[0])\n",
    "    done = False\n",
    "    print(len(trajectories), end=\"\\r\")\n",
    "    while not done:\n",
    "        action = model.predict(obs, deterministic=True)\n",
    "        obs, reward, done, info = env.step(action)\n",
    "\n",
    "        ep_acts.append(action[0][0])\n",
    "        ep_obs.append(obs[0])\n",
    "        ep_rews.append(reward[0])\n",
    "        ep_dones.append(done)\n",
    "        ep_infos.append({})\n",
    "\n",
    "    if ep_rews[-1] == 15.0: # Only keep trajectories that made it to the end flag (assume agent is not perfect)\n",
    "        trajectories.append(types.TrajectoryWithRew(acts=np.array(ep_acts),\n",
    "                                                    obs=np.array(ep_obs),\n",
    "                                                    rews=np.array(ep_rews),\n",
    "                                                    infos=ep_infos,\n",
    "                                                    terminal=True))\n",
    "    env.close()\n",
    "\n",
    "with open(\"expert_horizons_sticky_100.pkl\", \"wb\") as f:\n",
    "    pickle.dump(trajectories, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CODE TO COLLECT TRAJECTORIES FROM HUMAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open(\"human_demo_1.pkl\", \"rb\") as f:\n",
    "# with open('expert_horizons_sticky_100.pkl', 'rb') as f:\n",
    "    rollouts = pickle.load(f)\n",
    "\n",
    "ep = rollouts[0]\n",
    "for i in ep.obs:\n",
    "    screen = i[0]\n",
    "    for y in screen[::]:\n",
    "        for x in y[::]:\n",
    "            print(x, end=\"\\t\")\n",
    "        print(\"\")\n",
    "# print(ep.obs.shape)\n",
    "# print(ep.obs.dtype)\n",
    "# screen = ep.obs[0][0]\n",
    "# for y in screen[::]:\n",
    "#     for x in y[::]:\n",
    "#         print(x, end=\"\\t\")\n",
    "#     print(\"\")\n",
    "# print(ep.acts.shape)\n",
    "# print(len(ep.infos))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Allow execution to come from python side OR purely through Java\n",
    "# Do this with filewriter IF fps isnt affected\n",
    "import numpy as np \n",
    "from imitation.data import types, rollout\n",
    "import pickle\n",
    "import cv2\n",
    "\n",
    "\n",
    "ep_obs = []\n",
    "with open('src/Data/human/lvl-1/1/obs.txt') as f:\n",
    "    arr = (f.readline().strip().split(\", \"))\n",
    "    # print(arr)\n",
    "\n",
    "    stacked_obs = np.zeros([4, 84, 84])\n",
    "    # obs = []\n",
    "    # for j in range((int)(arr[1])):\n",
    "    #     row = []\n",
    "    #     for k in range((int)(arr[2])):\n",
    "    #         row.append((int)(f.readline().strip()))\n",
    "    #     obs.append(row)\n",
    "    #     # print(obs)\n",
    "    # obs = np.array(obs, dtype=np.uint8)\n",
    "\n",
    "    #     # self.obs[self.obs == 0] = 0 # sky \n",
    "    # obs[obs == 17] = 25 # ground\n",
    "    # obs[obs == 18] = 50 # stair block\n",
    "\n",
    "    # obs[obs == 56] = 75 # flag pole   \n",
    "    # obs[obs == 55] = 75 # flag top\n",
    "\n",
    "    # obs[obs == 34] = 100 # pipe\n",
    "    # obs[obs == 35] = 100\n",
    "    # obs[obs == 36] = 100\n",
    "    # obs[obs == 37] = 100\n",
    "\n",
    "    # obs[obs == 22] = 125 # break block\n",
    "    # obs[obs == 24] = 150 # coint block\n",
    "\n",
    "    # obs[obs == 2] = 175 # enemy \n",
    "    # obs[obs == 22] = 125 # break block\n",
    "    # obs[obs == 24] = 150 # coint block\n",
    "      \n",
    "    # obs[obs == 2] = 175 # enemy \n",
    "\n",
    "    # obs[obs == 12] = 200 # mushroom\n",
    "    # obs[obs == 30] = 225 # coin\n",
    "\n",
    "    # obs[obs == 99] = 255 # mario\n",
    "    # obs[obs == 97] = 245 # mario left\n",
    "\n",
    "\n",
    "    # obs = np.moveaxis(obs, -1, 0) # Swap x and y for readability\n",
    "    # # print(obs)\n",
    "\n",
    "\n",
    "    # obs = cv2.resize(obs, [80, 80], interpolation=cv2.INTER_AREA)\n",
    "    # obs = np.pad(obs, [[2,2],[2,2]])\n",
    "    # obs = obs.reshape([1, 84, 84])\n",
    "\n",
    "    # s = np.zeros([4, 84, 84])\n",
    "\n",
    "    # s[0] = stacked_obs[1]\n",
    "    # s[1] = stacked_obs[2]\n",
    "    # s[2] = stacked_obs[3]\n",
    "    # s[3] = obs\n",
    "\n",
    "    # ep_obs.append(s)\n",
    "    # stacked_obs = s\n",
    "\n",
    "    stride = 0\n",
    "    print(arr)\n",
    "\n",
    "    for i in range((int)(arr[0])):\n",
    "        obs = []\n",
    "        for j in range((int)(arr[1])):\n",
    "            row = []\n",
    "            for k in range((int)(arr[2])):\n",
    "                row.append((int)(f.readline().strip()))\n",
    "            obs.append(row)\n",
    "        # print(obs)\n",
    "        obs = np.array(obs, dtype=np.uint8)\n",
    "\n",
    "        # self.obs[self.obs == 0] = 0 # sky \n",
    "        obs[obs == 17] = 25 # ground\n",
    "        obs[obs == 18] = 50 # stair block\n",
    "\n",
    "        obs[obs == 56] = 75 # flag pole   \n",
    "        obs[obs == 55] = 75 # flag top\n",
    "\n",
    "        obs[obs == 34] = 100 # pipe\n",
    "        obs[obs == 35] = 100\n",
    "        obs[obs == 36] = 100\n",
    "        obs[obs == 37] = 100\n",
    "\n",
    "        obs[obs == 22] = 125 # break block\n",
    "        obs[obs == 24] = 150 # coint block\n",
    "\n",
    "        obs[obs == 2] = 175 # enemy \n",
    "        obs[obs == 22] = 125 # break block\n",
    "        obs[obs == 24] = 150 # coint block\n",
    "        \n",
    "        obs[obs == 2] = 175 # enemy \n",
    "\n",
    "        obs[obs == 12] = 200 # mushroom\n",
    "        obs[obs == 30] = 225 # coin\n",
    "\n",
    "        obs[obs == 99] = 255 # mario\n",
    "        obs[obs == 97] = 245 # mario left\n",
    "\n",
    "\n",
    "        obs = np.moveaxis(obs, -1, 0) # Swap x and y for readability\n",
    "        # print(obs)\n",
    "\n",
    "\n",
    "        obs = cv2.resize(obs, [80, 80], interpolation=cv2.INTER_AREA)\n",
    "        obs = np.pad(obs, [[2,2],[2,2]])\n",
    "        obs = obs.reshape([1, 84, 84])\n",
    "\n",
    "        s = np.zeros([4, 84, 84])\n",
    "\n",
    "        s[0] = stacked_obs[1]\n",
    "        s[1] = stacked_obs[2]\n",
    "        s[2] = stacked_obs[3]\n",
    "        s[3] = obs\n",
    "\n",
    "        if stride % 2 == 0 and stride > 0:\n",
    "            ep_obs.append(s)\n",
    "            stacked_obs = s\n",
    "\n",
    "        stride += 1\n",
    "\n",
    "    print(stacked_obs)\n",
    "\n",
    "\n",
    "ep_acts = []\n",
    "with open('src/Data/human/lvl-1/1/acts.txt') as f:\n",
    "    arr = (f.readline().strip().split(\", \"))\n",
    "    stride = 0\n",
    "    potential_acts = []\n",
    "    # print(arr)\n",
    "    for i in range((int)(arr[0])):\n",
    "        \n",
    "        act = []\n",
    "        for j in range((int)(arr[1])):\n",
    "            act.append((int)(f.readline().strip()))\n",
    "        act = (np.array(act)==1)\n",
    "\n",
    "        # convert for action array to action \n",
    "        action = 0\n",
    "        if np.all(act == [False, False, False, False, True]):\n",
    "            action = 1 # Jump\n",
    "        elif np.all(act == [True, False, False, False, False]):\n",
    "            action = 2 # Left\n",
    "        elif np.all(act == [True, False, False, True, False]):\n",
    "            action = 3 # Left Run\n",
    "        elif np.all(act == [True, False, False, False, True]):\n",
    "            action = 4 # Left Jump\n",
    "        elif np.all(act == [True, False, False, True, True]):\n",
    "            action = 5 # Left Run Jump\n",
    "        elif np.all(act == [False, True, False, False, False]):\n",
    "            action = 6 # Right\n",
    "        elif np.all(act == [False, True, False, True, False]):\n",
    "            action = 7 # Right Run\n",
    "        elif np.all(act == [False, True, False, False, True]):\n",
    "            action = 8 # Right Jump\n",
    "        elif np.all(act == [False, True, False, True, True]):\n",
    "            action = 9 # Right Run Jump\n",
    "\n",
    "        potential_acts.append(action)\n",
    "\n",
    "        if stride % 2  == 0 and stride > 0:\n",
    "            \n",
    "            if 1 in potential_acts:\n",
    "                ep_acts.append(1)\n",
    "            elif 4 in potential_acts:\n",
    "                ep_acts.append(4)\n",
    "            elif 5 in potential_acts:\n",
    "                ep_acts.append(5)\n",
    "            elif 8 in potential_acts:\n",
    "                ep_acts.append(8)\n",
    "            elif 9 in potential_acts:\n",
    "                ep_acts.append(9)\n",
    "            else:\n",
    "                ep_acts.append(np.bincount(potential_acts).argmax())\n",
    "            print(potential_acts, ep_acts[-1])\n",
    "            potential_acts = []\n",
    "        stride += 1\n",
    "\n",
    "\n",
    "ep_infos = []\n",
    "ep_rews = []\n",
    "with open('src/Data/human/lvl-1/1/infos.txt') as f:\n",
    "    arr = (f.readline().strip().split(\", \"))\n",
    "    stride = 0\n",
    "\n",
    "    # print(arr)\n",
    "    for i in range((int)(arr[0])):\n",
    "        info = []en(ep_acts) == 0 or\n",
    "        for j in range((int)(arr[1])):\n",
    "            info.append((float)(f.readline().strip()))\n",
    "        # print(info)\n",
    "\n",
    "        if stride % 2 == 0 and stride > 0:\n",
    "            ep_infos.append(info)\n",
    "            ep_rews.append(0.0) # for now discard reward as it is not used in BC or GAIL\n",
    "        stride += 1\n",
    "   \n",
    "if len(ep_acts) == len(ep_obs):\n",
    "    ep_acts.pop()\n",
    "    ep_rews.pop()\n",
    "    ep_infos.pop()\n",
    "\n",
    "print(len(ep_obs))\n",
    "print(len(ep_acts))\n",
    "print(len(ep_infos))\n",
    "print(len(ep_rews))\n",
    "\n",
    "trajectories = []\n",
    "trajectories.append(types.TrajectoryWithRew(acts=np.array(ep_acts),\n",
    "                                            obs=np.array(ep_obs, dtype=np.uint8),\n",
    "                                            rews=np.array(ep_rews),\n",
    "                                            infos=ep_infos,\n",
    "                                            terminal=True))\n",
    "                                    \n",
    "with open(\"human_demo_1_2skip.pkl\", \"wb\") as f:\n",
    "    pickle.dump(trajectories, f)\n",
    "\n",
    "\n",
    "# # print(ep_acts.shape)\n",
    "# # # print(ep_infos)\n",
    "# # print(ep_obs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"human_demo_1_4skip.pkl\", \"rb\") as f:\n",
    "    rollouts = pickle.load(f)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imitation.algorithms import bc\n",
    "from mario_env import MarioEnv\n",
    "from stable_baselines3.common.vec_env import VecFrameStack, VecTransposeImage, DummyVecEnv\n",
    "from gym.wrappers import ResizeObservation\n",
    "import pickle\n",
    "from imitation.data import types, rollout\n",
    "\n",
    "\n",
    "\n",
    "# Generate environment and wrap it\n",
    "env = MarioEnv(render=False)\n",
    "env = ResizeObservation(env, 84)\n",
    "env = DummyVecEnv([lambda: env])\n",
    "env = VecFrameStack(env, n_stack=4)\n",
    "env = VecTransposeImage(env, skip=False)\n",
    "\n",
    "# Load trajectories\n",
    "# with open(\"expert_horizons_sticky_100.pkl\", \"rb\") as f:\n",
    "with open(\"human_demo_1_2skip.pkl\", \"rb\") as f:\n",
    "    rollouts = pickle.load(f)\n",
    "\n",
    "transitions = rollout.flatten_trajectories(rollouts) # flatten into unordered obs, action, next_obs tuples\n",
    "\n",
    "# Set up BC trainer model\n",
    "bc_trainer = bc.BC(\n",
    "    observation_space=env.observation_space,\n",
    "    action_space=env.action_space,\n",
    "    demonstrations=transitions,\n",
    ")\n",
    "\n",
    "# Train agent\n",
    "for i in range(50):\n",
    "    bc_trainer.train(n_epochs=10000)\n",
    "    bc_trainer.save_policy(\"agents/bc_human/bc_2skip_3_\" + str((i+1)*10000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imitation.algorithms import bc\n",
    "from mario_env import MarioEnv\n",
    "\n",
    "\n",
    "from imitation.algorithms.adversarial.gail import GAIL\n",
    "from imitation.rewards.reward_nets import BasicRewardNet\n",
    "from imitation.util.networks import RunningNorm\n",
    "from imitation.util.util import make_vec_env\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv, VecFrameStack, VecTransposeImage\n",
    "import pickle\n",
    "\n",
    "from gym.wrappers import ResizeObservation\n",
    "\n",
    "import gym\n",
    "from stable_baselines3.common.callbacks import CheckpointCallback\n",
    "from imitation.data import rollout, types\n",
    "\n",
    "\n",
    "\n",
    "# Generate environment and wrap it\n",
    "env = MarioEnv(render=False, horizons=True, starts=False)\n",
    "env = ResizeObservation(env, 84)\n",
    "env = DummyVecEnv([lambda: env])\n",
    "env = VecFrameStack(env, n_stack=4)\n",
    "env = VecTransposeImage(env, skip=False)\n",
    "\n",
    "with open(\"human_demo_1_2skip.pkl\", \"rb\") as f:\n",
    "    rollouts = pickle.load(f)\n",
    "\n",
    "transitions = rollout.flatten_trajectories(rollouts)\n",
    "\n",
    "\n",
    "learner = PPO(\n",
    "    env=env,\n",
    "    policy=\"CnnPolicy\",\n",
    "    batch_size=64,\n",
    "    ent_coef=0.0,\n",
    "    learning_rate=0.0003,\n",
    "    n_epochs=10,\n",
    "    \n",
    ")\n",
    "\n",
    "reward_net = BasicRewardNet(\n",
    "    env.observation_space, env.action_space, normalize_input_layer=RunningNorm\n",
    ")\n",
    "gail_trainer = GAIL(\n",
    "    demonstrations=transitions,\n",
    "    demo_batch_size=512, # EDITED normally 1024\n",
    "    gen_replay_buffer_capacity=2048,\n",
    "    n_disc_updates_per_round=4,\n",
    "    venv=env,\n",
    "    gen_algo=learner,\n",
    "    reward_net=reward_net,\n",
    ")\n",
    "\n",
    "# learner_rewards_before_training, _ = evaluate_policy(\n",
    "#     learner, env, 100, return_episode_rewards=True\n",
    "# )\n",
    "\n",
    "# gail_trainer.train(3000)\n",
    "\n",
    "for i in range(4*1000):\n",
    "    gail_trainer.train(25000)  # Note: set to 300000 for better results\n",
    "    gail_trainer.gen_algo.save(\"agents/gail/gail_expert_PC_2skip_\" + str( (i+1) * 25000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "index 1107 is out of bounds for axis 0 with size 1107",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [2], line 114\u001b[0m\n\u001b[1;32m    111\u001b[0m s \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[1;32m    112\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mnot\u001b[39;00m done:\n\u001b[1;32m    113\u001b[0m     \u001b[39m# action = model.predict(obs, deterministic=True)\u001b[39;00m\n\u001b[0;32m--> 114\u001b[0m     action \u001b[39m=\u001b[39m [ep\u001b[39m.\u001b[39;49macts[s]]\n\u001b[1;32m    115\u001b[0m     obs, reward, done, info \u001b[39m=\u001b[39m env\u001b[39m.\u001b[39mstep(action)\n\u001b[1;32m    116\u001b[0m     s\u001b[39m+\u001b[39m\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m\n",
      "\u001b[0;31mIndexError\u001b[0m: index 1107 is out of bounds for axis 0 with size 1107"
     ]
    }
   ],
   "source": [
    "from stable_baselines3 import DQN, A2C, PPO\n",
    "from mario_env import MarioEnv\n",
    "from stable_baselines3.common.vec_env import VecFrameStack, VecTransposeImage, DummyVecEnv\n",
    "from gym.wrappers import ResizeObservation\n",
    "import time\n",
    "import os\n",
    "from imitation.algorithms import bc\n",
    "import numpy as np\n",
    "import pickle \n",
    "\n",
    "# Generate environment and wrap it\n",
    "env = MarioEnv(render=True, starts = False, sticky=False, timer=45)\n",
    "env = ResizeObservation(env, 84)\n",
    "env = DummyVecEnv([lambda: env])\n",
    "env = VecFrameStack(env, n_stack=4)\n",
    "env = VecTransposeImage(env, skip=False)\n",
    "\n",
    "# model = bc.reconstruct_policy(\"saved_agents/bc_policy_100epoch_expert\") # Load BC agent\n",
    "# model = bc.reconstruct_policy(\"agents/bc_human/bc_4skip_2_90000\")\n",
    "# model = bc.reconstruct_policy(\"agents/bc_human/bc_0skip_30000\")\n",
    "# model = PPO.load(\"saved_agents/gail_expert_PC_5450000.zip\") # Load GAIL agent\n",
    "model = PPO.load(\"agents/gail/gail_expert_PC_2skip_1500000.zip\")\n",
    "\n",
    "# model = bc.reconstruct_policy(\"agents/bc_human/bc_2skip_3_220000\")\n",
    "# obs = env.reset()\n",
    "\n",
    "# screen = obs[0][3]\n",
    "# for y in screen[::]:\n",
    "#     for x in y[::]:\n",
    "#         print(x, end=\"\\t\")\n",
    "#     print(\"\")\n",
    "\n",
    "# print(obs.shape)\n",
    "# print(np.sum(obs[0][0]))\n",
    "# print(np.sum(obs[0][1]))\n",
    "# print(np.sum(obs[0][2]))\n",
    "# print(np.sum(obs[0][3]))\n",
    "\n",
    "# action = model.predict(obs)\n",
    "# obs, reward, done, info = env.step(action)\n",
    "\n",
    "# screen = obs[0][3]\n",
    "# for y in screen[::]:\n",
    "#     for x in y[::]:\n",
    "#         print(x, end=\"\\t\")\n",
    "#     print(\"\")\n",
    "\n",
    "\n",
    "# print(np.sum(obs[0][0]))\n",
    "# print(np.sum(obs[0][1]))\n",
    "# print(np.sum(obs[0][2]))\n",
    "# print(np.sum(obs[0][3]))\n",
    "\n",
    "# action = model.predict(obs)\n",
    "# obs, reward, done, info = env.step(action)\n",
    "\n",
    "# screen = obs[0][3]\n",
    "# for y in screen[::]:\n",
    "#     for x in y[::]:\n",
    "#         print(x, end=\"\\t\")\n",
    "#     print(\"\")\n",
    "\n",
    "\n",
    "# print(np.sum(obs[0][0]))\n",
    "# print(np.sum(obs[0][1]))\n",
    "# print(np.sum(obs[0][2]))\n",
    "# print(np.sum(obs[0][3]))\n",
    "\n",
    "# action = model.predict(obs)\n",
    "# obs, reward, done, info = env.step(action)\n",
    "\n",
    "# screen = obs[0][3]\n",
    "# for y in screen[::]:\n",
    "#     for x in y[::]:\n",
    "#         print(x, end=\"\\t\")\n",
    "#     print(\"\")\n",
    "\n",
    "\n",
    "# print(np.sum(obs[0][0]))\n",
    "# print(np.sum(obs[0][1]))\n",
    "# print(np.sum(obs[0][2]))\n",
    "# print(np.sum(obs[0][3]))\n",
    "\n",
    "# action = model.predict(obs)\n",
    "# obs, reward, done, info = env.step(action)\n",
    "\n",
    "# print(np.sum(obs[0][0]))\n",
    "# print(np.sum(obs[0][1]))\n",
    "# print(np.sum(obs[0][2]))\n",
    "# print(np.sum(obs[0][3]))\n",
    "\n",
    "# screen = obs[0][3]\n",
    "# for y in screen[::]:\n",
    "#     for x in y[::]:\n",
    "#         print(x, end=\"\\t\")\n",
    "#     print(\"\")\n",
    "\n",
    "# env.close()\n",
    "with open(\"human_demo_1_0skip.pkl\", \"rb\") as f:\n",
    "# with open('expert_horizons_sticky_100.pkl', 'rb') as f:\n",
    "    rollouts = pickle.load(f)\n",
    "ep = rollouts[0]\n",
    "\n",
    "episodes = 1\n",
    "for i in range(episodes):\n",
    "    obs = env.reset()\n",
    "    done = False\n",
    "    # env.step([6])\n",
    "    # env.step([0])\n",
    "\n",
    "    s = 0\n",
    "    while not done:\n",
    "        # action = model.predict(obs, deterministic=True)\n",
    "        action = [ep.acts[s]]\n",
    "        obs, reward, done, info = env.step(action)\n",
    "        s+=1\n",
    "    env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "\n",
    "with open(\"human_demo_1_4skip.pkl\", \"rb\") as f:\n",
    "# with open('expert_horizons_sticky_100.pkl', 'rb') as f:\n",
    "    rollouts = pickle.load(f)\n",
    "\n",
    "for i in range(100):\n",
    "\n",
    "    ep = rollouts[0]\n",
    "    print(ep.obs.shape)\n",
    "    screen = ep.obs[i][3]\n",
    "    for y in screen[::]:\n",
    "        for x in y[::]:\n",
    "            print(x, end=\"\\t\")\n",
    "        print(\"\")\n",
    "    print(ep.acts[i])\n",
    "    \n",
    "\n",
    "    # print(ep.obs.shape)\n",
    "\n",
    "    # print(np.sum(ep.obs[2][0]))\n",
    "    # print(np.sum(ep.obs[2][1]))\n",
    "    # print(np.sum(ep.obs[2][2]))\n",
    "    # print(np.sum(ep.obs[2][3]))\n",
    "    # print(ep.acts.shape)\n",
    "    # print(len(ep.infos))\n",
    "\n",
    "    # with open(\"human_demo_1.pkl\", \"rb\") as f:\n",
    "    # with open('expert_horizons_sticky_100.pkl', 'rb') as f:\n",
    "    #     rollouts = pickle.load(f)\n",
    "\n",
    "    # ep = rollouts[0]\n",
    "    # # print(ep.obs.shape)\n",
    "\n",
    "    # # print(np.sum(ep.obs[2][0]))\n",
    "    # # print(np.sum(ep.obs[2][1]))\n",
    "    # # print(np.sum(ep.obs[2][2]))\n",
    "    # # print(np.sum(ep.obs[2][3]))\n",
    "\n",
    "    # screen = ep.obs[i][3]\n",
    "    # for y in screen[::]:\n",
    "    #     for x in y[::]:\n",
    "    #         print(x, end=\"\\t\")\n",
    "    #     print(\"\")\n",
    "\n",
    "    # print(ep.acts[i])\n",
    "\n",
    "    print(\"----------------------\" + str(i) + \"-----------------\")\n",
    "    # print(ep.acts.shape)\n",
    "    # print(len(ep.infos))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Accidentally deleted code to connect to PythonController.java and run sample() to collect trajectories from human. Remake this code eventually!\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 ('venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "00dea995a6cfc0ea4f78e5fab6efed7a64f34d4129fdeb913f68b76b97c864d6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
