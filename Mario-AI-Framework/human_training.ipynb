{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-process trajectories collected by a human\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20\n"
     ]
    }
   ],
   "source": [
    "import numpy as np \n",
    "from imitation.data import types, rollout\n",
    "import pickle\n",
    "import cv2\n",
    "\n",
    "examples = [\"src/Data/human/lvl-1/2\",\n",
    "            'src/Data/human/lvl-1/3',\n",
    "            'src/Data/human/lvl-1/4',\n",
    "            'src/Data/human/lvl-1/5',\n",
    "            'src/Data/human/lvl-1/6',\n",
    "            'src/Data/human/lvl-1/7',\n",
    "            'src/Data/human/lvl-1/8',\n",
    "            'src/Data/human/lvl-1/9',\n",
    "            'src/Data/human/lvl-1/10',\n",
    "            'src/Data/human/lvl-1/11',\n",
    "            'src/Data/human/lvl-1/12',\n",
    "            'src/Data/human/lvl-1/13',\n",
    "            'src/Data/human/lvl-1/14',\n",
    "            'src/Data/human/lvl-1/15',\n",
    "            'src/Data/human/lvl-1/16',\n",
    "            'src/Data/human/lvl-1/17',\n",
    "            'src/Data/human/lvl-1/18',\n",
    "            'src/Data/human/lvl-1/19',\n",
    "            'src/Data/human/lvl-1/20',\n",
    "            'src/Data/human/lvl-1/21',\n",
    "]\n",
    "\n",
    "subsample_rate = 1\n",
    "horizons = True\n",
    "max_timestep = 1025\n",
    "\n",
    "trajectories = []\n",
    "\n",
    "for example in examples:\n",
    "\n",
    "    # Parse observations\n",
    "    # print('Parsing Observations')\n",
    "    ep_obs = []\n",
    "    with open(example + '/obs.txt') as f:\n",
    "        arr = (f.readline().strip().split(\", \")) # first line used as argument to know length of arrays\n",
    "        stacked_obs = np.zeros([4, 84, 84])\n",
    "        stride = -1 # start stride counter at -1 so that we keep the first sample\n",
    "        # print(arr)\n",
    "\n",
    "        # Parse java multidimensional array\n",
    "        for i in range((int)(arr[0])): \n",
    "            stride += 1\n",
    "\n",
    "            if stride % subsample_rate != 0: # subsample every s sample point\n",
    "                continue\n",
    "\n",
    "            \n",
    "            obs = []\n",
    "            for j in range((int)(arr[1])):\n",
    "                row = []\n",
    "                for k in range((int)(arr[2])):\n",
    "                    row.append((int)(f.readline().strip()))\n",
    "                obs.append(row)\n",
    "            obs = np.array(obs, dtype=np.uint8)\n",
    "\n",
    "            # Like in the environment, manually replace tile encoding with our state representation\n",
    "            # self.obs[self.obs == 0] = 0 # sky \n",
    "            obs[obs == 17] = 25 # ground\n",
    "            obs[obs == 18] = 50 # stair block\n",
    "\n",
    "            obs[obs == 56] = 75 # flag pole   \n",
    "            obs[obs == 55] = 75 # flag top\n",
    "\n",
    "            obs[obs == 34] = 100 # pipe\n",
    "            obs[obs == 35] = 100\n",
    "            obs[obs == 36] = 100\n",
    "            obs[obs == 37] = 100\n",
    "\n",
    "            obs[obs == 22] = 125 # break block\n",
    "            obs[obs == 24] = 150 # coint block\n",
    "\n",
    "            obs[obs == 2] = 175 # enemy \n",
    "            obs[obs == 22] = 125 # break block\n",
    "            obs[obs == 24] = 150 # coint block\n",
    "            \n",
    "            obs[obs == 2] = 175 # enemy \n",
    "\n",
    "            obs[obs == 12] = 200 # mushroom\n",
    "            obs[obs == 30] = 225 # coin\n",
    "\n",
    "            obs[obs == 99] = 255 # mario\n",
    "            obs[obs == 97] = 245 # mario left\n",
    "\n",
    "\n",
    "            obs = np.moveaxis(obs, -1, 0) # Swap x and y for readability\n",
    "            # print(obs)\n",
    "\n",
    "            # resize from 16x16 to 84x84\n",
    "            obs = cv2.resize(obs, [80, 80], interpolation=cv2.INTER_AREA)\n",
    "            obs = np.pad(obs, [[2,2],[2,2]])\n",
    "            obs = obs.reshape([1, 84, 84])\n",
    "\n",
    "\n",
    "            s = np.zeros([4, 84, 84]) # Stack the last 4 accepted frames over the first channel\n",
    "            s[0] = stacked_obs[1]\n",
    "            s[1] = stacked_obs[2]\n",
    "            s[2] = stacked_obs[3]\n",
    "            s[3] = obs\n",
    "\n",
    "            ep_obs.append(s)\n",
    "            stacked_obs = s\n",
    "\n",
    "        # print(stacked_obs)\n",
    "    # print(len(ep_obs))\n",
    "\n",
    "    # Parse Actions\n",
    "    # print('Parsing Actions')\n",
    "\n",
    "    ep_acts = []\n",
    "    with open(example + '/acts.txt') as f:\n",
    "        arr = (f.readline().strip().split(\", \")) # first line used as argument to know length of arrays\n",
    "        # print(arr)\n",
    "        stride = -1 # start stride counter at -1 so that we keep the first sample\n",
    "        potential_acts = [] # because of frame skipping we need to decide which action of the skipped frames to take\n",
    "\n",
    "        # Parse java multidimensional array\n",
    "        for i in range((int)(arr[0])):\n",
    "            stride += 1\n",
    "\n",
    "            act = []\n",
    "            for j in range((int)(arr[1])):\n",
    "                act.append((int)(f.readline().strip()))\n",
    "            act = (np.array(act)==1)\n",
    "\n",
    "            # convert for action array to action \n",
    "            action = 0\n",
    "            if np.all(act == [False, False, False, False, True]):\n",
    "                action = 1 # Jump\n",
    "            elif np.all(act == [True, False, False, False, False]):\n",
    "                action = 2 # Left\n",
    "            elif np.all(act == [True, False, False, True, False]):\n",
    "                action = 3 # Left Run\n",
    "            elif np.all(act == [True, False, False, False, True]):\n",
    "                action = 4 # Left Jump\n",
    "            elif np.all(act == [True, False, False, True, True]):\n",
    "                action = 5 # Left Run Jump\n",
    "            elif np.all(act == [False, True, False, False, False]):\n",
    "                action = 6 # Right\n",
    "            elif np.all(act == [False, True, False, True, False]):\n",
    "                action = 7 # Right Run\n",
    "            elif np.all(act == [False, True, False, False, True]):\n",
    "                action = 8 # Right Jump\n",
    "            elif np.all(act == [False, True, False, True, True]):\n",
    "                action = 9 # Right Run Jump\n",
    "\n",
    "            potential_acts.append(action)\n",
    "\n",
    "            if stride % subsample_rate != 0: # subsample every s sample point\n",
    "                continue\n",
    "\n",
    "                \n",
    "            # Manually override so that jump actions take precedent within skipped frames\n",
    "            if 1 in potential_acts:\n",
    "                ep_acts.append(1)\n",
    "            elif 4 in potential_acts:\n",
    "                ep_acts.append(4)\n",
    "            elif 5 in potential_acts:\n",
    "                ep_acts.append(5)\n",
    "            elif 8 in potential_acts:\n",
    "                ep_acts.append(8)\n",
    "            elif 9 in potential_acts:\n",
    "                ep_acts.append(9)\n",
    "            else: # Else, take the most frequent action\n",
    "                ep_acts.append(np.bincount(potential_acts).argmax())\n",
    "            # print(potential_acts, ep_acts[-1])\n",
    "\n",
    "            potential_acts = []\n",
    "    # print(len(ep_acts))\n",
    "\n",
    "\n",
    "    # Parse infos/rewards\n",
    "    # print('Parsing Infos/Rewards')\n",
    "    ep_infos = []\n",
    "    ep_rews = []\n",
    "    with open(example + '/infos.txt') as f:\n",
    "        arr = (f.readline().strip().split(\", \"))\n",
    "        stride = -1\n",
    "\n",
    "        # Parse java multidimensional array\n",
    "        for i in range((int)(arr[0])):\n",
    "            stride += 1\n",
    "\n",
    "            if stride % subsample_rate != 0: # subsample every s sample point\n",
    "                continue\n",
    "\n",
    "            info = []\n",
    "            for j in range((int)(arr[1])):\n",
    "                info.append((float)(f.readline().strip()))\n",
    "            # print(info)\n",
    "\n",
    "            ep_infos.append(info) # info not used in either as well but good to have for debugging\n",
    "            ep_rews.append(0.0) # for now discard reward as it is not used in BC or GAIL\n",
    "\n",
    "    # NOTE: len(obs) == len(acts) + 1 == len(infos) + 1 == len(rews) + 1\n",
    "    if len(ep_acts) == len(ep_obs):\n",
    "        ep_acts.pop()\n",
    "        ep_rews.pop()\n",
    "        ep_infos.pop()\n",
    "\n",
    "\n",
    "    if horizons:\n",
    "        if len(ep_acts) < max_timestep - 1:\n",
    "            \n",
    "            stacked_o = ep_obs[-1]\n",
    "\n",
    "        \n",
    "            for i in range(4):\n",
    "                stacked_o = ep_obs[-1]\n",
    "                o = np.ones([1, 84, 84]) * 85\n",
    "\n",
    "                s = np.zeros([4, 84, 84]) # Stack the last 4 accepted frames over the first channel\n",
    "                s[0] = stacked_obs[1]\n",
    "                s[1] = stacked_obs[2]\n",
    "                s[2] = stacked_obs[3]\n",
    "                s[3] = o\n",
    "\n",
    "                ep_acts.append(0)\n",
    "                ep_infos.append([])\n",
    "                ep_rews.append(0.0)\n",
    "                ep_obs.append(s)\n",
    "                \n",
    "            for i in range(max_timestep - len(ep_acts) - 4):\n",
    "\n",
    "                ep_obs.append(np.ones([4, 84, 84]) * 85)\n",
    "                ep_acts.append(0)\n",
    "                ep_infos.append([])\n",
    "                ep_rews.append(0.0)\n",
    "\n",
    "\n",
    "    trajectories.append(types.TrajectoryWithRew(acts=np.array(ep_acts),\n",
    "                                            obs=np.array(ep_obs, dtype=np.uint8),\n",
    "                                            rews=np.array(ep_rews),\n",
    "                                            infos=ep_infos,\n",
    "                                            terminal=True))\n",
    "\n",
    "print(len(trajectories))       \n",
    "\n",
    "with open(\"human_20ep_0skip_horizons.pkl\", \"wb\") as f:\n",
    "    pickle.dump(trajectories, f)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test trajectory by running it through the env sequentially"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines3 import DQN, A2C, PPO\n",
    "from mario_env import MarioEnv\n",
    "from stable_baselines3.common.vec_env import VecFrameStack, VecTransposeImage, DummyVecEnv\n",
    "from gym.wrappers import ResizeObservation\n",
    "import time\n",
    "import os\n",
    "from imitation.algorithms import bc\n",
    "import numpy as np\n",
    "import pickle \n",
    "\n",
    "# Generate environment and wrap it\n",
    "env = MarioEnv(render=True, starts = False, sticky=False, timer=45, skip=1)\n",
    "env = ResizeObservation(env, 84)\n",
    "env = DummyVecEnv([lambda: env])\n",
    "env = VecFrameStack(env, n_stack=4)\n",
    "env = VecTransposeImage(env, skip=False)\n",
    "\n",
    "\n",
    "\n",
    "with open(\"human_20ep_0skip.pkl\", \"rb\") as f:\n",
    "    rollouts = pickle.load(f)\n",
    "\n",
    "\n",
    "for episode in [rollouts[0]]:\n",
    "    obs = env.reset()\n",
    "    for step in range(len(episode.acts)):\n",
    "        action = [episode.acts[step]]\n",
    "        env.step(action)\n",
    "    env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training an Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imitation.algorithms import bc\n",
    "from mario_env import MarioEnv\n",
    "from stable_baselines3.common.vec_env import VecFrameStack, VecTransposeImage, DummyVecEnv\n",
    "from gym.wrappers import ResizeObservation\n",
    "import pickle\n",
    "from imitation.data import types, rollout\n",
    "\n",
    "\n",
    "\n",
    "# Generate environment and wrap it\n",
    "env = MarioEnv(render=False, skip=1)\n",
    "env = ResizeObservation(env, 84)\n",
    "env = DummyVecEnv([lambda: env])\n",
    "env = VecFrameStack(env, n_stack=4)\n",
    "env = VecTransposeImage(env, skip=False)\n",
    "\n",
    "# Load trajectories\n",
    "# with open(\"expert_horizons_sticky_100.pkl\", \"rb\") as f:\n",
    "with open(\"human_20ep_0skip.pkl\", \"rb\") as f:\n",
    "    rollouts = pickle.load(f)\n",
    "\n",
    "transitions = rollout.flatten_trajectories(rollouts) # flatten into unordered obs, action, next_obs tuples\n",
    "\n",
    "# Set up BC trainer model\n",
    "bc_trainer = bc.BC(\n",
    "    observation_space=env.observation_space,\n",
    "    action_space=env.action_space,\n",
    "    demonstrations=transitions,\n",
    ")\n",
    "\n",
    "# Train agent\n",
    "for i in range(10):\n",
    "    bc_trainer.train(n_epochs=100) # 100 epochs ~4 mins\n",
    "    bc_trainer.save_policy(\"agents/bc_human_20ep/bc_0skip_\" + str((i+1)*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imitation.algorithms import bc\n",
    "from mario_env import MarioEnv\n",
    "\n",
    "\n",
    "from imitation.algorithms.adversarial.gail import GAIL\n",
    "from imitation.rewards.reward_nets import BasicRewardNet\n",
    "from imitation.util.networks import RunningNorm\n",
    "from imitation.util.util import make_vec_env\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv, VecFrameStack, VecTransposeImage\n",
    "import pickle\n",
    "\n",
    "from gym.wrappers import ResizeObservation\n",
    "\n",
    "import gym\n",
    "from stable_baselines3.common.callbacks import CheckpointCallback\n",
    "from imitation.data import rollout, types\n",
    "\n",
    "\n",
    "# Generate environment and wrap it\n",
    "env = MarioEnv(render=False, horizons=True, starts=False, skip=1, max_timestep=1000)\n",
    "env = ResizeObservation(env, 84)\n",
    "env = DummyVecEnv([lambda: env])\n",
    "env = VecFrameStack(env, n_stack=4)\n",
    "env = VecTransposeImage(env, skip=False)\n",
    "\n",
    "with open(\"human_20ep_0skip_horizons.pkl\", \"rb\") as f:\n",
    "    rollouts = pickle.load(f)\n",
    "\n",
    "transitions = rollout.flatten_trajectories(rollouts)\n",
    "\n",
    "\n",
    "learner = PPO(\n",
    "    env=env,\n",
    "    policy=\"CnnPolicy\",\n",
    "    batch_size=64,\n",
    "    ent_coef=0.0,\n",
    "    learning_rate=0.0003,\n",
    "    n_epochs=10,\n",
    "    \n",
    ")\n",
    "\n",
    "reward_net = BasicRewardNet(\n",
    "    env.observation_space, env.action_space, normalize_input_layer=RunningNorm\n",
    ")\n",
    "gail_trainer = GAIL(\n",
    "    demonstrations=transitions,\n",
    "    demo_batch_size=512, # EDITED normally 1024\n",
    "    gen_replay_buffer_capacity=2048,\n",
    "    n_disc_updates_per_round=4,\n",
    "    venv=env,\n",
    "    gen_algo=learner,\n",
    "    reward_net=reward_net,\n",
    ")\n",
    "\n",
    "gail_trainer.train(25000)\n",
    "gail_trainer.gen_algo.save(\"agents/gail_human_20ep/gail\")\n",
    "\n",
    "# for i in range(4*10):\n",
    "#     gail_trainer.train(25000) \n",
    "#     gail_trainer.gen_algo.save(\"agents/gail_human_20ep/gail_0skip_\" + str( (i+1) * 25000))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing agents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines3 import DQN, A2C, PPO\n",
    "from mario_env import MarioEnv\n",
    "from stable_baselines3.common.vec_env import VecFrameStack, VecTransposeImage, DummyVecEnv\n",
    "from gym.wrappers import ResizeObservation\n",
    "import time\n",
    "import os\n",
    "from imitation.algorithms import bc\n",
    "import numpy as np\n",
    "import pickle \n",
    "\n",
    "# Generate environment and wrap it\n",
    "env = MarioEnv(render=True, starts = False, sticky=False, timer=45, skip=1)\n",
    "env = ResizeObservation(env, 84)\n",
    "env = DummyVecEnv([lambda: env])\n",
    "env = VecFrameStack(env, n_stack=4)\n",
    "env = VecTransposeImage(env, skip=False)\n",
    "\n",
    "model = bc.reconstruct_policy(\"agents/bc_human_20ep/bc_0skip_400\") # Load BC agent\n",
    "# model = PPO.load(\"saved_agents/gail_expert_PC_5450000.zip\") # Load GAIL agent\n",
    "\n",
    "episodes = 1\n",
    "for i in range(episodes):\n",
    "    obs = env.reset()\n",
    "    done = False\n",
    "    while not done:\n",
    "        action = model.predict(obs, deterministic=True)\n",
    "        obs, reward, done, info = env.step(action)\n",
    "\n",
    "        screen = obs[0][3]\n",
    "        for y in screen[::]:\n",
    "            for x in y[::]:\n",
    "                print(x, end=\"\\t\")\n",
    "            print(\"\")\n",
    "        print(\"---\")\n",
    "\n",
    "    env.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 ('venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "00dea995a6cfc0ea4f78e5fab6efed7a64f34d4129fdeb913f68b76b97c864d6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
