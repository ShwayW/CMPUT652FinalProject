{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random initial setup tests\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test that jpype and java environment is setup properly\n",
    "\n",
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "import jpype\n",
    "\n",
    "if not jpype.isJVMStarted():\n",
    "    jpype.startJVM(jpype.getDefaultJVMPath(), \"-ea\")\n",
    "\n",
    "jpype.addClassPath(\"/home/mikeg/Documents/CMPUT652FinalProject/CMPUT652FinalProject/Mario-AI-Framework/src\") # TODO: figure how to use relative path here\n",
    "main = jpype.JClass('PythonController')\n",
    "\n",
    "# Run a sample to test jpype is set up correctly\n",
    "main.reset(False)\n",
    "result = main.step([True, False, False, False, True])\n",
    "\n",
    "# Print out screen observation\n",
    "for i in range(result.observation.length):\n",
    "            for j in range(result.observation[i].length):\n",
    "                print(result.observation[i][j], end=\" \")\n",
    "            print(\"\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test if current environment correctly implements OpenAI Gym standards\n",
    "\n",
    "from stable_baselines3.common.env_checker import check_env\n",
    "from mario_env import MarioEnv\n",
    "from stable_baselines3.common.vec_env import VecFrameStack, VecTransposeImage, DummyVecEnv\n",
    "from gym.wrappers import ResizeObservation\n",
    "\n",
    "env = MarioEnv(render=False)\n",
    "\n",
    "# It will check your custom environment and output additional warnings if needed\n",
    "check_env(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample 100 random actions in the environment\n",
    "\n",
    "from mario_env import MarioEnv\n",
    "import numpy as np\n",
    "\n",
    "env = MarioEnv()\n",
    "\n",
    "for i in range(1):\n",
    "    done = False\n",
    "    env.reset()\n",
    "    # while not done:\n",
    "    for i in range(20):\n",
    "        obs, reward, done, info = env.step(6 + np.random.randint(4)) # sample randomly from right-inputs only\n",
    "        for j in obs:\n",
    "            for k in j:\n",
    "                print(k[0], end=\"\\t\")\n",
    "            print(\"\")\n",
    "        print(\"---\" + str(i) + \"---\")\n",
    "        # print(info)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# --- Training an Agent ---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines3 import DQN, A2C, PPO\n",
    "from mario_env import MarioEnv\n",
    "from stable_baselines3.common.vec_env import VecFrameStack, VecTransposeImage, DummyVecEnv\n",
    "from gym.wrappers import ResizeObservation\n",
    "from stable_baselines3.common.callbacks import CheckpointCallback\n",
    "\n",
    "# Set up a callback to save the model periodically\n",
    "checkpoint_callback = CheckpointCallback(\n",
    "  save_freq=25000,\n",
    "  save_path=\"./agents/\",\n",
    "  name_prefix=\"ppo_resize_sticky20\",\n",
    "  save_replay_buffer=True,\n",
    "  save_vecnormalize=True,\n",
    ")\n",
    "\n",
    "env = MarioEnv(render=False, sticky=False, starts = False)\n",
    "\n",
    "# Perform some wrapping on the environment\n",
    "print(env.observation_space.shape)\n",
    "env = ResizeObservation(env, 84) # Resize observations from 16x16 to 84x84\n",
    "print(env.observation_space.shape)\n",
    "env = DummyVecEnv([lambda: env]) # turn into vectorized environment\n",
    "print(env.observation_space.shape)\n",
    "env = VecFrameStack(env, n_stack=4) # stack vectorized environment by 4 frames\n",
    "print(env.observation_space.shape)\n",
    "env = VecTransposeImage(env, skip=False) # needed for images to properly be processed by CnnPolicy\n",
    "print(env.observation_space.shape)\n",
    "\n",
    "\n",
    "model = PPO(\"CnnPolicy\", env=env, verbose=1)\n",
    "# model.learn(total_timesteps=1000000, callback=checkpoint_callback)\n",
    "model.learn(total_timesteps=250000) # Train without checkpointing\n",
    "model.save(\"model\") # save model a second time to model.zip so its easy to test new models right away"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# --- Testing an Agent ---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines3 import DQN, A2C, PPO\n",
    "from mario_env import MarioEnv\n",
    "from stable_baselines3.common.vec_env import VecFrameStack, VecTransposeImage, DummyVecEnv\n",
    "from gym.wrappers import ResizeObservation\n",
    "from imitation.algorithms import bc\n",
    "import time\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "# Generate environment and wrap it\n",
    "env = MarioEnv(render=True, starts=False, horizons=False, sticky=False)\n",
    "env = ResizeObservation(env, 84)\n",
    "env = DummyVecEnv([lambda: env])\n",
    "env = VecFrameStack(env, n_stack=4)\n",
    "env = VecTransposeImage(env, skip=False)\n",
    "\n",
    "model = PPO.load(\"model\") # Load last trained model\n",
    "\n",
    "\n",
    "# Load current best agents:\n",
    "# model = PPO.load(\"saved_agents/ppo_resize_sticky_500000_steps.zip\") # Current best PPO agent (can do with sticky actions)\n",
    "# model = PPO.load(\"saved_agents/ppo_tile_4skip_fwd_goomba_200000_steps.zip\") # goomba agent\n",
    "# model = bc.reconstruct_policy(\"saved_agents/bc_policy_100epoch_expert\") # Load BC agent\n",
    "model = PPO.load(\"saved_agents/gail_expert_PC_5450000.zip\") # best GAIL so far (horizon on training examples)\n",
    "\n",
    "\n",
    "episodes = 1\n",
    "for i in range(episodes):\n",
    "    obs = env.reset()\n",
    "    done = False\n",
    "    t = 0\n",
    "    while not done:\n",
    "    # for i in range(12):\n",
    "        action = model.predict(obs, deterministic=True)\n",
    "        obs, reward, done, info = env.step(action)\n",
    "    \n",
    "        screen = obs[0][3]\n",
    "        for y in screen[::]:\n",
    "            for x in y[::]:\n",
    "                print(x, end=\"\\t\")\n",
    "            print(\"\")\n",
    "\n",
    "        print(action)\n",
    "        print(reward) \n",
    "        print(action)\n",
    "        print(info)\n",
    "        # time.sleep(0.5)\n",
    "        print(\"------\" + str(t)+\"-----\")\n",
    "        t+=1\n",
    "    env.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SAMPLE OF CUSTOM CNN POLICY (NOT USED ANYMORE)\n",
    "\n",
    "# import gym\n",
    "# import torch as th\n",
    "# import torch.nn as nn\n",
    "\n",
    "# from stable_baselines3 import DQN\n",
    "# from stable_baselines3.common.torch_layers import BaseFeaturesExtractor\n",
    "\n",
    "# from mario_env import MarioEnv\n",
    "# from stable_baselines3.common.vec_env import VecFrameStack, VecTransposeImage\n",
    "# from  stable_baselines3.common.vec_env import DummyVecEnv\n",
    "\n",
    "\n",
    "# class CustomCNN(BaseFeaturesExtractor):\n",
    "#     \"\"\"\n",
    "#     :param observation_space: (gym.Space)\n",
    "#     :param features_dim: (int) Number of features extracted.\n",
    "#         This corresponds to the number of unit for the last layer.\n",
    "#     \"\"\"\n",
    "\n",
    "#     def __init__(self, observation_space: gym.spaces.Box, features_dim: int = 256):\n",
    "#         super(CustomCNN, self).__init__(observation_space, features_dim)\n",
    "#         # We assume CxHxW images (channels first)\n",
    "#         # Re-ordering will be done by pre-preprocessing or wrapper\n",
    "#         n_input_channels = observation_space.shape[0]\n",
    "#         self.cnn = nn.Sequential(\n",
    "#             nn.Conv2d(n_input_channels, 32, kernel_size=8, stride=4, padding=0),\n",
    "#             nn.ReLU(),\n",
    "#             nn.Conv2d(32, 64, kernel_size=4, stride=2, padding=1),\n",
    "#             nn.ReLU(),\n",
    "#             nn.Flatten(),\n",
    "#         )\n",
    "\n",
    "#         # Compute shape by doing one forward pass\n",
    "#         with th.no_grad():\n",
    "#             n_flatten = self.cnn(\n",
    "#                 th.as_tensor(observation_space.sample()[None]).float()\n",
    "#             ).shape[1]\n",
    "\n",
    "#         self.linear = nn.Sequential(nn.Linear(n_flatten, features_dim), nn.ReLU())\n",
    "\n",
    "#     def forward(self, observations: th.Tensor) -> th.Tensor:\n",
    "#         return self.linear(self.cnn(observations))\n",
    "\n",
    "\n",
    "# env = MarioEnv(render=False)\n",
    "# print(env.observation_space.shape)\n",
    "# env = DummyVecEnv([lambda: env])\n",
    "# print(env.observation_space.shape)\n",
    "# env = VecFrameStack(env, n_stack=4)\n",
    "# print(env.observation_space.shape)\n",
    "# env = VecTransposeImage(env, skip=True)\n",
    "# print(env.observation_space.shape)\n",
    "\n",
    "# policy_kwargs = dict(\n",
    "#     features_extractor_class=CustomCNN,\n",
    "#     features_extractor_kwargs=dict(features_dim=128),\n",
    "# )\n",
    "# model = DQN(\"CnnPolicy\", env, policy_kwargs=policy_kwargs, verbose=1)\n",
    "# model.learn(50000)\n",
    "# model.save('cnn_50k_timesteps')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "b4c4209b22c3460bb6de3eac9245934e56ba70d0216febe174dc12e72cf2cf61"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
